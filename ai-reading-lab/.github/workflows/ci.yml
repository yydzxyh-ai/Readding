name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11, 3.12]

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Run tests
      run: |
        python -m pytest tests/ -v
    
    - name: Run quality tests
      run: |
        python -m pytest tests/test_snapshots.py tests/test_aggregate.py -v
    
    - name: Run linting
      run: |
        ruff check ai_lab/ tests/
        black --check ai_lab/ tests/
    
    - name: Run type checking
      run: |
        mypy ai_lab/

  quality-check:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Validate quality metrics
      run: |
        python -c "
        from ai_lab.utils import calculate_coverage_metrics, evaluate_summary_quality, faithfulness_proxy
        print('✓ Quality metrics functions imported successfully')
        
        # Test with sample data
        test_summaries = [
            {
                'title': 'Test',
                'tl_dr': 'Test summary',
                'contributions': ['test'],
                'methods': ['test'],
                'results': ['test'],
                'limitations': ['test'],
                'tags': ['test']
            }
        ]
        
        coverage = calculate_coverage_metrics(test_summaries)
        print(f'✓ Coverage metrics calculated: {len(coverage)} metrics')
        
        quality = evaluate_summary_quality(test_summaries[0], 'test context')
        print(f'✓ Quality evaluation completed: {len(quality)} metrics')
        
        faithfulness = faithfulness_proxy('test summary', 'test context')
        print(f'✓ Faithfulness proxy calculated: {faithfulness}')
        "
    
    - name: Test aggregation functions
      run: |
        python -c "
        from ai_lab.aggregate import merge_json_summaries, render_markdown_digest
        print('✓ Aggregation functions imported successfully')
        
        # Test with empty input
        empty_result = merge_json_summaries('nonexistent/*.json')
        assert empty_result == [], 'Empty result should be empty list'
        print('✓ Empty input handling works')
        
        # Test markdown rendering
        md = render_markdown_digest([])
        assert '# Weekly Digest —' in md, 'Should contain title'
        print('✓ Markdown rendering works')
        "

  weekly-digest:
    runs-on: ubuntu-latest
    needs: [test, quality-check]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Create sample data for testing
      run: |
        mkdir -p data summaries/json
        echo "Sample PDF content" > data/sample.txt
        echo '{"title": "Test Paper", "tl_dr": "Test summary", "contributions": ["test"], "methods": ["test"], "results": ["test"], "limitations": ["test"], "tags": ["test"], "source_path": "test.pdf"}' > summaries/json/test.json
    
    - name: Test full pipeline (without LLM calls)
      run: |
        echo "Testing aggregation step..."
        python -m ai_lab.aggregate --json_glob "summaries/json/*.json" --out summaries/WEEKLY_DIGEST.md --verbose
        echo "✓ Pipeline test completed"
    
    - name: Validate output
      run: |
        if [ -f "summaries/WEEKLY_DIGEST.md" ]; then
          echo "✓ Weekly digest generated successfully"
          head -5 summaries/WEEKLY_DIGEST.md
        else
          echo "✗ Weekly digest not generated"
          exit 1
        fi
